{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/adbar/trafilatura\n",
    "# https://adrien.barbaresi.eu/blog/using-sitemaps-crawl-websites.html\n",
    "# https://trafilatura.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the links using the sitemap\n",
    "# https://1001suns.com/robots.txt\n",
    "# https://1001suns.com/wp-sitemap.xml\n",
    "# https://1001suns.com/sitemap.xml\n",
    "# https://trafilatura.readthedocs.io/en/latest/usage-python.html\n",
    "from trafilatura import sitemaps\n",
    "\n",
    "url = 'https://1001suns.com'\n",
    "url = 'https://1001suns.com/sitemap_post'\n",
    "#url = 'https://karlsruhe.digital'\n",
    "\n",
    "print(\"url\", url)\n",
    "links = sitemaps.sitemap_search(url)\n",
    "if False:\n",
    "    for link in links:\n",
    "        print(len(links), \"sitemap_search:\", link)\n",
    "        links.extend(sitemaps.sitemap_search(link)) # stays the same\n",
    "        links = sorted(list(set(links))) # make unique\n",
    "    print(\"links:\", len(links))\n",
    "\n",
    "outfile = \"trafilatura_links_long_\" + url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\") + \".txt\"\n",
    "with open(outfile, 'w') as f:\n",
    "    for link in links:\n",
    "        f.write(link + \"\\n\")\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = trafilatura.settings.use_config('trafilatura-settings.cfg')\n",
    "#print(myconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://programtalk.com/python-more-examples/trafilatura.sitemaps.handle_link/\n",
    "trafilatura.sitemaps.handle_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://1001suns.com/'\n",
    "\n",
    "to_visit, known_urls = trafilatura.spider.focused_crawler(\n",
    "    url, \n",
    "    max_seen_urls=10, max_known_urls=100000,\n",
    "    config=myconfig,\n",
    ")\n",
    "print(\"to_visit:\", to_visit)\n",
    "print(\"known_urls:\", sorted(known_urls))\n",
    "\n",
    "# to_visit, known_urls = focused_crawler(homepage, max_seen_urls=10, max_known_urls=100000, todo=to_visit, known_links=known_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = trafilatura.fetch_url(url)\n",
    "# print(downloaded) # html\n",
    "j = trafilatura.extract( \n",
    "    downloaded, \n",
    "    output_format='json',\n",
    "    # include_formatting=True,\n",
    "    # include_links=True,\n",
    "    include_comments=False, include_tables=False, no_fallback=True,\n",
    "    #favor_precision=True\n",
    ") # text only\n",
    "print(json.dumps(json.loads(j), indent=4))\n",
    "\n",
    "trafilatura.bare_extraction(downloaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.thepythoncode.com/article/extract-all-website-links-python\n",
    "# https://www.thepythoncode.com/code/extract-all-website-links-python\n",
    "# pip3 install requests bs4 colorama\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
    "main_url = \"https://1001suns.com/sitemap_post/\"\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url,headers=headers).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl(url, max_urls=30):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 30.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        if total_urls_visited > max_urls:\n",
    "            break\n",
    "        crawl(link, max_urls=max_urls)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
    "    parser.add_argument(\"--url\",        help=\"The URL to extract links from.\",  default=main_url,   type=str)\n",
    "    parser.add_argument(\"--max-urls\",   help=\"Number of max URLs to crawl.\",    default=1000,        type=int)\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "    args, _unknown_args = parser.parse_known_args()\n",
    "    url = args.url\n",
    "    max_urls = args.max_urls\n",
    "    print(\"url     :\", url)\n",
    "    print(\"max_urls:\", max_urls)\n",
    "\n",
    "    crawl(url, max_urls=max_urls)\n",
    "\n",
    "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "    print(\"[+] Total External links:\", len(external_urls))\n",
    "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
    "    print(\"[+] Total crawled URLs:\", max_urls)\n",
    "\n",
    "    domain_name = urlparse(url).netloc\n",
    "    print(\"domain_name:\", domain_name)\n",
    "\n",
    "    # save the internal links to a file\n",
    "    print(\"save the internal links to a file...\")\n",
    "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
    "        for internal_link in internal_urls:\n",
    "            print(internal_link.strip(), file=f)\n",
    "\n",
    "    # save the external links to a file\n",
    "    print(\"save the external links to a file...\")\n",
    "    with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
    "        for external_link in external_urls:\n",
    "            print(external_link.strip(), file=f)\n",
    "            \n",
    "    print(\"all done.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b56f5dbaca42e5d06fe2344edc63f7213399c22cbe4fa6238526ea3d79eb9046"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
